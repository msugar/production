{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with parquet files\n",
    "\n",
    "## Objective\n",
    "\n",
    "+ In this assignment, we will use the data downloaded with the module `data_manager` to create features.\n",
    "\n",
    "(11 pts total)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "+ This notebook assumes that price data is available to you in the environment variable `PRICE_DATA`. If you have not done so, then execute the notebook `production_2_data_engineering.ipynb` to create this data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Load the environment variables using dotenv. (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below.\n",
    "\n",
    "# Option 1: Jupyter Notebook magic commands\n",
    "\n",
    "#     %load_ext dotenv\n",
    "#     %dotenv\n",
    "\n",
    "\n",
    "# Option 2: Plain Python code\n",
    "\n",
    "# Load environment variable using dotenv\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the PRICE_DATA environment variable\n",
    "PRICE_DATA = os.getenv('PRICE_DATA')\n",
    "PRICE_DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the query planning option on to prevent message\n",
    "import dask\n",
    "dask.config.set({'dataframe.query-planning': True})\n",
    "    \n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Load the environment variable `PRICE_DATA`.\n",
    "+ Use [glob](https://docs.python.org/3/library/glob.html) to find the path of all parquet files in the directory `PRICE_DATA`.\n",
    "\n",
    "(1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Write your code below.\n",
    "\n",
    "# Retrieve the PRICE_DATA environment variable\n",
    "PRICE_DATA = os.getenv('PRICE_DATA')\n",
    "assert os.path.isdir(PRICE_DATA), f\"'{PRICE_DATA=}' is not a valid directory\"\n",
    "\n",
    "# Get all *.parquet files and directories recursively\n",
    "parquet_paths = glob(os.path.join(PRICE_DATA, \"**\", \"*.parquet\"), recursive=True)\n",
    "\n",
    "# Filter to keep only files (exclude directories)\n",
    "parquet_files = [path for path in parquet_paths if os.path.isfile(path)]\n",
    "assert len(parquet_files) == 11207, f\"Expected 11207 files, but found {len(parquet_files)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each ticker and using Dask, do the following:\n",
    "\n",
    "+ Add lags for variables Close and Adj_Close.\n",
    "+ Add returns based on Adjusted Close:\n",
    "    \n",
    "    - `returns`: (Adj Close / Adj Close_lag) - 1\n",
    "\n",
    "+ Add the following range: \n",
    "\n",
    "    - `hi_lo_range`: this is the day's High minus Low.\n",
    "\n",
    "+ Assign the result to `dd_feat`.\n",
    "\n",
    "(4 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read all parquet files into a single Dask DataFrame\n",
    "ddf = dd.read_parquet(parquet_files).set_index('ticker')\n",
    "\n",
    "# Provides Dask with a template of the expected output structure, \n",
    "# so it knows the columns and data types without computing the \n",
    "# entire operation immediately.\n",
    "# Not strictly necessary, but it's a nice-to-have.\n",
    "column_types = {\n",
    "    'Date': 'datetime64[ns, UTC]',\n",
    "    'Adj Close': float,\n",
    "    'Close': float,\n",
    "    'High': float,\n",
    "    'Low': float,\n",
    "    'Open': float,\n",
    "    'Volume': np.int64,\n",
    "    'sector': 'string[pyarrow]',\n",
    "    'subsector': 'string[pyarrow]',\n",
    "    'year': 'int32',\n",
    "    'Close_lag': float,\n",
    "    'Adj_Close_lag': float,\n",
    "    'hi_lo_range': float,\n",
    "    'returns': float\n",
    "}\n",
    "meta_df = pd.DataFrame({col: pd.Series(dtype=dt) for col, dt in column_types.items()})\n",
    "\n",
    "# Option 1: Add features using chain of apply(), lambda, and assign()\n",
    "dd_feat = (\n",
    "    ddf.groupby('ticker', group_keys=False)\n",
    "    .apply(\n",
    "        lambda x: x.sort_values('Date').assign(\n",
    "            # Add lags for 'Close' and 'Adj_Close'\n",
    "            Close_lag = x['Close'].shift(1),\n",
    "            Adj_Close_lag = x['Adj Close'].shift(1),\n",
    "\n",
    "            # Calculate the daily high-low range\n",
    "            hi_lo_range = x['High'] - x['Low']\n",
    "        ).assign(\n",
    "            # Calculate returns based on Adjusted Close\n",
    "            returns = lambda x: x['Adj Close'] / x['Adj_Close_lag'] - 1\n",
    "        )\n",
    "        , meta = meta_df\n",
    "    )\n",
    ")\n",
    "\n",
    "# Option 2: Add features with apply() and externally defined function.\n",
    "# (See my Student Notes at the bottom of this notebook for details.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Convert the Dask data frame to a pandas data frame. \n",
    "+ Add a rolling average return calculation with a window of 10 days.\n",
    "+ *Tip*: Consider using `.rolling(10).mean()`.\n",
    "\n",
    "(3 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below.\n",
    "\n",
    "# Convert the Dask DataFrame to a Pandas DataFrame (takes 2m36s)\n",
    "pd_feat = dd_feat.compute()\n",
    "\n",
    "# Calculate the 10-day rolling average return using the Pandas dataframe (takes 1s)\n",
    "pd_feat['avg_return_10d'] = pd_feat.groupby('ticker')['returns'].rolling(window=10).mean().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please comment:\n",
    "\n",
    "+ Was it necessary to convert to pandas to calculate the moving average return?\n",
    "+ Would it have been better to do it in Dask? Why?\n",
    "\n",
    "(1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Was it necessary to convert to Pandas to calculate the moving average return?\n",
    "\n",
    "No, it wasn't strictly necessary to convert to Pandas to calculate the moving average return. Dask has support for rolling operations, so we could have calculated the moving average return directly within Dask without converting.\n",
    "\n",
    "### Would it have been better to do it in Dask? Why?\n",
    "\n",
    "No, in this particular case it would not, because the data is small enough to fit in memory and the computation is fast [1].\n",
    "\n",
    "In addition, using Dask for rolling window operations is not as convenient as doing it in Pandas. With Dask, you should ensure that the partition sizes you choose are large enough to avoid boundary issues, but keep in mind that larger partitions can begin to slow down your computations. The data should also be index-aligned to ensure that itâ€™s sorted in the correct order. Dask uses the index to determine which rows are adjacent to one another, so ensuring proper sort order is critical for the correct execution of any calculations on the data. [2]\n",
    "\n",
    "References:\n",
    "- [1] Dask. (n.d.). *Dask DataFrame*. Retrieved October 27, 2024, from [https://docs.dask.org/en/stable/dataframe.html#when-not-to-use-dask-dataframes](https://docs.dask.org/en/stable/dataframe.html#when-not-to-use-dask-dataframes)\n",
    "- [2] Daniel, J. C. (2019). *Data science with Python and Dask* (p. 161). Manning Publications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteria\n",
    "\n",
    "The [rubric](./assignment_1_rubric_clean.xlsx) contains the criteria for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Information\n",
    "\n",
    "ðŸš¨ **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** ðŸš¨ for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "### Submission Parameters:\n",
    "* Submission Due Date: `HH:MM AM/PM - DD/MM/YYYY`\n",
    "* The branch name for your repo should be: `assignment-1`\n",
    "* What to submit for this assignment:\n",
    "    * This Jupyter Notebook (assignment_1.ipynb) should be populated and should be the only change in your pull request.\n",
    "* What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    * Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "Checklist:\n",
    "- [ x ] Created a branch with the correct naming convention.\n",
    "- [ x ] Ensured that the repository is public.\n",
    "- [ x ] Reviewed the PR description guidelines and adhered to them.\n",
    "- [ x ] Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack at `#cohort-3-help`. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To verify, display the first results\n",
    "print(pd_feat.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prevent ValueError: cannot reindex on an axis with duplicate labels\n",
    "pd_feat = pd_feat.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd_feat[[]'ticker', 'Date', 'returns', 'avg_return_10d']].head(2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Notes\n",
    "\n",
    "*Option 2: Add features with apply() and externally defined function*\n",
    "\n",
    "This option only works when the function is defined outside the Jupyter notebook, otherwise Dask throws an error: \n",
    "```\n",
    "Function ... may not be deterministically hashed by cloudpickle\n",
    "```\n",
    "Here are the steps to use this approach:\n",
    "1. Define the function in its own source file, outside the notebook. For example, in `${SRC_DIR}/feature_engineering.py`:\n",
    "    ```python\n",
    "    # For each ticker, add lags, returns, and high-low range\n",
    "    def add_features(df):\n",
    "        # Sort by date if not already sorted\n",
    "        #df = df.sort_index()\n",
    "        \n",
    "        # Add lags for 'Close' and 'Adj_Close'\n",
    "        df['Close_lag'] = df['Close'].shift(1)\n",
    "        df['Adj_Close_lag'] = df['Adj Close'].shift(1)\n",
    "        \n",
    "        # Calculate returns based on Adjusted Close\n",
    "        df['returns'] = (df['Adj Close'] / df['Adj_Close_lag']) - 1\n",
    "        \n",
    "        # Calculate the daily high-low range\n",
    "        df['hi_lo_range'] = df['High'] - df['Low']\n",
    "        \n",
    "        return df\n",
    "    ```\n",
    "\n",
    "2. In the notebook, import the externally defined function and apply it to each group of the Dask dataframe:\n",
    "    ```python\n",
    "    import sys\n",
    "    sys.path.append(os.getenv('SRC_DIR'))\n",
    "\n",
    "    from feature_engineering import add_features\n",
    "\n",
    "    dd_feat = ddf.groupby('ticker', group_keys=False).apply(add_features, meta=ddf)\n",
    "    ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
